{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCY6UbkkI9_N"
      },
      "source": [
        "# Style Transfer\n",
        "\n",
        "<img src=\"https://i0.wp.com/chelseatroy.com/wp-content/uploads/2018/12/neural_style_transfer.png?resize=768%2C311&ssl=1\">\n",
        "\n",
        "La idea de este trabajo final es reproducir el siguiente paper:\n",
        "\n",
        "https://arxiv.org/pdf/1508.06576.pdf\n",
        "\n",
        "El objetivo es transferir el estilo de una imagen dada a otra imagen distinta. \n",
        "\n",
        "Como hemos visto en clase, las primeras capas de una red convolucional se activan ante la presencia de ciertos patrones vinculados a detalles muy pequeños.\n",
        "\n",
        "A medida que avanzamos en las distintas capas de una red neuronal convolucional, los filtros se van activando a medida que detectan patrones de formas cada vez mas complejos.\n",
        "\n",
        "Lo que propone este paper es asignarle a la activación de las primeras capas de una red neuronal convolucional (por ejemplo VGG19) la definición del estilo y a la activación de las últimas capas de la red neuronal convolucional, la definición del contenido.\n",
        "\n",
        "La idea de este paper es, a partir de dos imágenes (una que aporte el estilo y otra que aporte el contenido) analizar cómo es la activación de las primeras capas para la imagen que aporta el estilo y cómo es la activación de las últimas capas de la red convolucional para la imagen que aporta el contenido. A partir de esto se intentará sintetizar una imagen que active los filtros de las primeras capas que se activaron con la imagen que aporta el estilo y los filtros de las últimas capas que se activaron con la imagen que aporta el contenido.\n",
        "\n",
        "A este procedimiento se lo denomina neural style transfer.\n",
        "\n",
        "# En este trabajo se deberá leer el paper mencionado y en base a ello, entender la implementación que se muestra a continuación y contestar preguntas sobre la misma.\n",
        "\n",
        "# Una metodología posible es hacer una lectura rápida del paper (aunque esto signifique no entender algunos detalles del mismo) y luego ir analizando el código y respondiendo las preguntas. A medida que se planteen las preguntas, volviendo a leer secciones específicas del paper terminará de entender los detalles que pudieran haber quedado pendientes.\n",
        "\n",
        "Lo primero que haremos es cargar dos imágenes, una que aporte el estilo y otra que aporte el contenido. A tal fin utilizaremos imágenes disponibles en la web."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "disable_eager_execution()"
      ],
      "metadata": {
        "id": "XowWZKKS1FyA"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyHsa2t0SxZi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a7977eb-9cf4-424f-966c-e314f70cdcb1"
      },
      "source": [
        "# Imagen para estilo\n",
        "!wget https://upload.wikimedia.org/wikipedia/commons/0/04/Alfred_Sisley_001.jpg\n",
        "\n",
        "# Imagen para contenido\n",
        "!wget https://upload.wikimedia.org/wikipedia/commons/4/44/Cape_Mountain_Zebra_%28Equus_zebra_zebra%29_%2832510283536%29.jpg\n",
        "\n",
        "# Creamos el directorio para los archivos de salida\n",
        "!mkdir /content/resultados"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-24 19:49:22--  https://upload.wikimedia.org/wikipedia/commons/0/04/Alfred_Sisley_001.jpg\n",
            "Resolving upload.wikimedia.org (upload.wikimedia.org)... 103.102.166.240, 2620:0:862:ed1a::2:b\n",
            "Connecting to upload.wikimedia.org (upload.wikimedia.org)|103.102.166.240|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 500207 (488K) [image/jpeg]\n",
            "Saving to: ‘Alfred_Sisley_001.jpg.1’\n",
            "\n",
            "Alfred_Sisley_001.j 100%[===================>] 488.48K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-02-24 19:49:25 (28.2 MB/s) - ‘Alfred_Sisley_001.jpg.1’ saved [500207/500207]\n",
            "\n",
            "--2023-02-24 19:49:25--  https://upload.wikimedia.org/wikipedia/commons/4/44/Cape_Mountain_Zebra_%28Equus_zebra_zebra%29_%2832510283536%29.jpg\n",
            "Resolving upload.wikimedia.org (upload.wikimedia.org)... 103.102.166.240, 2620:0:862:ed1a::2:b\n",
            "Connecting to upload.wikimedia.org (upload.wikimedia.org)|103.102.166.240|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7082351 (6.8M) [image/jpeg]\n",
            "Saving to: ‘Cape_Mountain_Zebra_(Equus_zebra_zebra)_(32510283536).jpg.5’\n",
            "\n",
            "Cape_Mountain_Zebra 100%[===================>]   6.75M  29.1MB/s    in 0.2s    \n",
            "\n",
            "2023-02-24 19:49:25 (29.1 MB/s) - ‘Cape_Mountain_Zebra_(Equus_zebra_zebra)_(32510283536).jpg.5’ saved [7082351/7082351]\n",
            "\n",
            "mkdir: cannot create directory ‘/content/resultados’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIxH20o2eFoc"
      },
      "source": [
        "from tensorflow.keras.utils import load_img, save_img, img_to_array\n",
        "import numpy as np\n",
        "from scipy.optimize import fmin_l_bfgs_b\n",
        "import time\n",
        "import argparse\n",
        "\n",
        "from keras.applications import vgg19\n",
        "from keras import backend as K\n",
        "from pathlib import Path"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLkV1bnFl_tK"
      },
      "source": [
        "# Definimos las imagenes que vamos a utilizar, y el directorio de salida\n",
        "\n",
        "base_image_path = Path(\"/content/Zebra_finch_group.png\")\n",
        "style_reference_image_path = Path(\"/content/Alfred_Sisley_001.jpg\")\n",
        "result_prefix = Path(\"/content/output\")\n",
        "iterations = 100"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz2PeGfpeYzj"
      },
      "source": [
        "# 1) En base a lo visto en el paper ¿Qué significan los parámetros definidos en la siguiente celda?\n",
        "\n",
        "Respuesta: Style wieght es el beta de las loss function mientras que Content wieght es el alpha. \n",
        "Estos parametros deifinen el peso que se le va a dar al estilo que proviene de una pintura o al contenido de la fotografia para producir la imagen de salida\n",
        "\n",
        "total_variation_weight define content_weight/style_weight, alpha/beta,\n",
        "es decir la relacion entre el estilo y el contenido en la imagen de salida.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9Dt3aaEmJWS"
      },
      "source": [
        "total_variation_weight = 0.1\n",
        "style_weight = 10\n",
        "content_weight = 1"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQQJOhCVuse6"
      },
      "source": [
        "# Definimos el tamaño de las imágenes a utilizar\n",
        "width, height = load_img(base_image_path).size\n",
        "img_nrows = 400\n",
        "img_ncols = int(width * img_nrows / height)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg2ct-8agm1E"
      },
      "source": [
        "# 2) Explicar qué hace la siguiente celda. En especial las últimas dos líneas de la función antes del return. ¿Por qué?\n",
        "\n",
        "Ayuda: https://keras.io/applications/\n",
        "\n",
        "Respuesta: La funcion carga la imagen y la convierte en el \n",
        "tamaño definido anteriormente a un np array y luego lo expande\n",
        "de 3D a 4D. VGG19 convierte las imagenes de entrada de RGB a BGR y centra cada canal de color a 0\n",
        "con respecto al ImageNet dataset.  Utiliza los valores: [103.939, 116.779, 123.68] para el centrado. \n",
        "Convirtiendo la imagen en un tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAkljg4zuzYd"
      },
      "source": [
        "def preprocess_image(image_path):\n",
        "    img = load_img(image_path, target_size=(img_nrows, img_ncols))\n",
        "    img = img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = vgg19.preprocess_input(img)\n",
        "    return img"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTf0YDSagt10"
      },
      "source": [
        "# 3) Habiendo comprendido lo que hace la celda anterior, explique de manera muy concisa qué hace la siguiente celda. ¿Qué relación tiene con la celda anterior?\n",
        "\n",
        "Respuesta: Esta funcion convierte el tensor en una imagen nuevamente, remueve el centrado en cero de cada canal.\n",
        "Utiliza los mismos valores que la funcion anterior [103.939, 116.779, 123.68] para convertir\n",
        "el BGR a RGB."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5LaTrsAu14z"
      },
      "source": [
        "def deprocess_image(x):\n",
        "    x = x.reshape((img_nrows, img_ncols, 3))\n",
        "    # Remove zero-center by mean pixel\n",
        "    x[:, :, 0] += 103.939\n",
        "    x[:, :, 1] += 116.779\n",
        "    x[:, :, 2] += 123.68\n",
        "    # 'BGR'->'RGB'\n",
        "    x = x[:, :, ::-1]\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYNio09mu4S3"
      },
      "source": [
        "# get tensor representations of our images\n",
        "# K.variable convierte un numpy array en un tensor, para \n",
        "base_image = K.variable(preprocess_image(base_image_path))\n",
        "style_reference_image = K.variable(preprocess_image(style_reference_image_path))"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1Lbw02Uu--o"
      },
      "source": [
        "combination_image = K.placeholder((1, img_nrows, img_ncols, 3))"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJEi0YI3Uzrm"
      },
      "source": [
        "Aclaración:\n",
        "\n",
        "La siguiente celda sirve para procesar las tres imagenes (contenido, estilo y salida) en un solo batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGO_jGFfvEbF"
      },
      "source": [
        "# combine the 3 images into a single Keras tensor\n",
        "input_tensor = K.concatenate([base_image,\n",
        "                              style_reference_image,\n",
        "                              combination_image], axis=0)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdG59VRavHGB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c85af933-4577-4285-dd29-ec1e45d9f1e4"
      },
      "source": [
        "# build the VGG19 network with our 3 images as input\n",
        "# the model will be loaded with pre-trained ImageNet weights\n",
        "model = vgg19.VGG19(input_tensor=input_tensor,\n",
        "                    weights='imagenet', include_top=False)\n",
        "print('Model loaded.')\n",
        "\n",
        "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
        "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70-vs_jZkKVc"
      },
      "source": [
        "# 4) En la siguientes celdas:\n",
        "\n",
        "- ¿Qué es la matriz de Gram?¿Para qué se usa?\n",
        "- ¿Por qué se permutan las dimensiones de x?\n",
        "\n",
        "\n",
        "Respuesta: Gram matrix es el producto punto entre los vectores features aplanados. El resultado\n",
        "nos brinda informacion  entre los vectores. Cuanto mayor sea el producto mayor es la correlacion \n",
        "entre features. Las dimensiones se permutan para asegurar que la matriz sea computada en las dimensiones correctas.\n",
        "El producto punto sea realiza sobre las dimensiones del canal que son la ultima dimension  de un\n",
        "CNN feature map. Por esta razon, se pasa la ultima  dim a la primera dimension, para que la matriz de gram resulte mas facil de computar.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1FODPATvJ1k"
      },
      "source": [
        "def gram_matrix(x):\n",
        "    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
        "    gram = K.dot(features, K.transpose(features))\n",
        "    return gram"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBQkKFY0Rbx-"
      },
      "source": [
        "# 5) Losses:\n",
        "\n",
        "Explicar qué mide cada una de las losses en las siguientes tres celdas.\n",
        "\n",
        "Rta: style_losss: representa la perdida entre el estilo de la imagen generada y \n",
        "el estilo de referencia (de la imagen de entrada)\n",
        "Depende de la matrices gram entre los feature maps de la imagen de referencia y la imagen de entrada.\n",
        "\n",
        "content_loss: Es la diferencia entre el output de la capa mas alta de la imagen de entrada y la misma capa en la imagen generada. \n",
        "Ya que las capas mas altas en una CNN contienen la informacion de la estructura de la imagen.\n",
        "\n",
        "total_variation_loss: suaviza la variaciones de pixeles entre pixeles cercanos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-Gt0ahWvN6q"
      },
      "source": [
        "def style_loss(style, combination):\n",
        "    assert K.ndim(style) == 3\n",
        "    assert K.ndim(combination) == 3\n",
        "    S = gram_matrix(style)\n",
        "    C = gram_matrix(combination)\n",
        "    channels = 3\n",
        "    size = img_nrows * img_ncols\n",
        "    return K.sum(K.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCqnju5RvQCo"
      },
      "source": [
        "def content_loss(base, combination):\n",
        "    return K.sum(K.square(combination - base))\n"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udEp5h31vRnY"
      },
      "source": [
        "def total_variation_loss(x):\n",
        "    assert K.ndim(x) == 4\n",
        "    a = K.square(\n",
        "        x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :])\n",
        "    b = K.square(\n",
        "        x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :])\n",
        "    return K.sum(K.pow(a + b, 1.25))\n"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-65vcinbvTZ0"
      },
      "source": [
        "# Armamos la loss total\n",
        "loss = K.variable(0.0)\n",
        "layer_features = outputs_dict['block5_conv2']\n",
        "base_image_features = layer_features[0, :, :, :]\n",
        "combination_features = layer_features[2, :, :, :]\n",
        "loss = loss + content_weight * content_loss(base_image_features,\n",
        "                                            combination_features)\n",
        "\n",
        "feature_layers = ['block1_conv1', 'block2_conv1',\n",
        "                  'block3_conv1', 'block4_conv1',\n",
        "                  'block5_conv1']\n",
        "for layer_name in feature_layers:\n",
        "    layer_features = outputs_dict[layer_name]\n",
        "    style_reference_features = layer_features[1, :, :, :] \n",
        "    combination_features = layer_features[2, :, :, :]\n",
        "    sl = style_loss(style_reference_features, combination_features)\n",
        "    loss = loss + (style_weight / len(feature_layers)) * sl\n",
        "loss = loss + total_variation_weight * total_variation_loss(combination_image)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbz4n1OhvV2K"
      },
      "source": [
        "\n",
        "\n",
        "grads = K.gradients(loss, combination_image)\n",
        "\n",
        "outputs = [loss]\n",
        "if isinstance(grads, (list, tuple)):\n",
        "    outputs += grads\n",
        "else:\n",
        "    outputs.append(grads)\n",
        "\n",
        "f_outputs = K.function([combination_image], outputs)"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FGr82spY1Dd3"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JbydbOaVcvU"
      },
      "source": [
        "# 6) Explique el propósito de las siguientes tres celdas. ¿Qué hace la función fmin_l_bfgs_b? ¿En qué se diferencia con la implementación del paper? ¿Se puede utilizar alguna alternativa?\n",
        "\n",
        "Respuesta: En primer lugar, se evaluan los gradientes y la loss con respecto a la imagen generada.\n",
        "\n",
        "Luego, Evaluator  devuelve la loss y el gradiente en dos funciones separadas pero \n",
        "dependientes. Finalmente, se utiliza fmin_l_bfgs como optimizador que sirve para minimizar\n",
        "una funcion multivariada.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVE1_qemvZeN"
      },
      "source": [
        "def eval_loss_and_grads(x):\n",
        "    x = x.reshape((1, img_nrows, img_ncols, 3))\n",
        "    outs = f_outputs([x])\n",
        "    loss_value = outs[0]\n",
        "    if len(outs[1:]) == 1:\n",
        "        grad_values = outs[1].flatten().astype('float64')\n",
        "    else:\n",
        "        grad_values = np.array(outs[1:]).flatten().astype('float64')\n",
        "    return loss_value, grad_values\n",
        "\n",
        "# this Evaluator class makes it possible\n",
        "# to compute loss and gradients in one pass\n",
        "# while retrieving them via two separate functions,\n",
        "# \"loss\" and \"grads\". This is done because scipy.optimize\n",
        "# requires separate functions for loss and gradients,\n",
        "# but computing them separately would be inefficient."
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbl9roIgvdb1"
      },
      "source": [
        "class Evaluator(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.loss_value = None\n",
        "        self.grads_values = None\n",
        "\n",
        "    def loss(self, x):\n",
        "        assert self.loss_value is None\n",
        "        loss_value, grad_values = eval_loss_and_grads(x)\n",
        "        self.loss_value = loss_value\n",
        "        self.grad_values = grad_values\n",
        "        return self.loss_value\n",
        "\n",
        "    def grads(self, x):\n",
        "        assert self.loss_value is not None\n",
        "        grad_values = np.copy(self.grad_values)\n",
        "        self.loss_value = None\n",
        "        self.grad_values = None\n",
        "        return grad_values"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb0yOEl-WOE6"
      },
      "source": [
        "# 7) Ejecute la siguiente celda y observe las imágenes de salida en cada iteración."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n31YBwCVvhAI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "outputId": "9f153e37-2e58-482a-b84d-656c7413df71"
      },
      "source": [
        "evaluator = Evaluator()\n",
        "\n",
        "# run scipy-based optimization (L-BFGS) over the pixels of the generated image\n",
        "# so as to minimize the neural style loss\n",
        "x = preprocess_image(base_image_path)\n",
        "\n",
        "for i in range(iterations):\n",
        "    print('Start of iteration2', i)\n",
        "    start_time = time.time()\n",
        "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n",
        "                                     fprime=evaluator.grads, maxfun=20)\n",
        "    print('Current loss value:', min_val)\n",
        "    # save current generated image\n",
        "    img = deprocess_image(x.copy())\n",
        "    fname = result_prefix / ('iterations2_%d.png' % i)\n",
        "    save_img(fname, img)\n",
        "    end_time = time.time()\n",
        "    print('Image saved as', fname)\n",
        "    print('Iteration %d completed in %ds' % (i, end_time - start_time))"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start of iteration2 0\n",
            "Current loss value: 8981788000.0\n",
            "Image saved as /content/output/iterations2_0.png\n",
            "Iteration 0 completed in 8s\n",
            "Start of iteration2 1\n",
            "Current loss value: 4828688400.0\n",
            "Image saved as /content/output/iterations2_1.png\n",
            "Iteration 1 completed in 9s\n",
            "Start of iteration2 2\n",
            "Current loss value: 3693020400.0\n",
            "Image saved as /content/output/iterations2_2.png\n",
            "Iteration 2 completed in 8s\n",
            "Start of iteration2 3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-109-9adc5a0f441a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Start of iteration2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n\u001b[0m\u001b[1;32m     11\u001b[0m                                      fprime=evaluator.grads, maxfun=20)\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Current loss value:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[0;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[1;32m    195\u001b[0m             'maxls': maxls}\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[0m\u001b[1;32m    198\u001b[0m                            **opts)\n\u001b[1;32m    199\u001b[0m     d = {'grad': res['jac'],\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_x_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;31m# Overwriting results in undefined behaviour because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;31m# fun(self.x) will change self.x, with the two no longer linked.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-106-c0eccc4316a2>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_loss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-105-a5e92c09b58a>\u001b[0m in \u001b[0;36meval_loss_and_grads\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meval_loss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_nrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_ncols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   4579\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4581\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4582\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4583\u001b[0m         output_structure = tf.nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1479\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1480\u001b[0m         \u001b[0mrun_metadata_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1481\u001b[0;31m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0m\u001b[1;32m   1482\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m                                                run_metadata_ptr)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c5C1ttzRzkre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkiJtofbWWy1"
      },
      "source": [
        "# 8) Generar imágenes para distintas combinaciones de pesos de las losses. Explicar las diferencias. (Adjuntar las imágenes generadas como archivos separados.)\n",
        "\n",
        "Respuesta: Utilizo los parametros alternados:\n",
        "\n",
        "total_variation_weight = 0.1\n",
        "\n",
        "style_weight = 10\n",
        "\n",
        "content_weight = 1\n",
        "\n",
        "\n",
        "En esta caso, contrario a lo que sucedia antes, la imagen generada dara un mayor peso al estilo que al contenido. \n",
        "\n",
        "\n",
        "# 9) Cambiar las imágenes de contenido y estilo por unas elegidas por usted. Adjuntar el resultado.\n",
        "\n",
        "Respuesta: Adjunto en el repo"
      ]
    }
  ]
}